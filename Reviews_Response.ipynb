{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# preprocessed data\n",
    "#from datasets.twitter import data\n",
    "import data\n",
    "import data_utils\n",
    "import os\n",
    "\n",
    "print(tf.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, idx_q, idx_a = data.load_data(PATH='')\n",
    "(trainX, trainY), (testX, testY), (validX, validY) = data_utils.split_dataset(idx_q, idx_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "[ 10   2   8  88   4  48   3 410  15 114   6  13   9  71   6  36  67   9\n",
      "  27   3  41   2  17  26   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0]\n",
      "40\n",
      "2153\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# parameters \n",
    "xseq_len = trainX.shape[-1]\n",
    "print(xseq_len)\n",
    "print(trainY[1])\n",
    "yseq_len = trainY.shape[-1]\n",
    "print(yseq_len)\n",
    "batch_size = 24\n",
    "xvocab_size = len(metadata['idx2w'])  \n",
    "yvocab_size = xvocab_size\n",
    "emb_dim = 1024\n",
    "print (xvocab_size)\n",
    "import seq2seq_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(329, 40)\n",
      "(70, 40)\n",
      "(70, 40)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(validX.shape)\n",
    "print(testX.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<log> Building Graph </log>"
     ]
    }
   ],
   "source": [
    "setattr(tf.contrib.rnn.GRUCell, '__deepcopy__', lambda self, _: self)\n",
    "setattr(tf.contrib.rnn.BasicLSTMCell, '__deepcopy__', lambda self, _: self)\n",
    "setattr(tf.contrib.rnn.MultiRNNCell, '__deepcopy__', lambda self, _: self)\n",
    "model = seq2seq_wrapper.Seq2Seq(xseq_len=xseq_len,\n",
    "                               yseq_len=yseq_len,\n",
    "                               xvocab_size=xvocab_size,\n",
    "                               yvocab_size=yvocab_size,\n",
    "                               ckpt_path='ckpt/twitter/',\n",
    "                               emb_dim=emb_dim,\n",
    "                               num_layers=3\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch_gen = data_utils.rand_batch_gen(validX, validY, 24)\n",
    "train_batch_gen = data_utils.rand_batch_gen(trainX, trainY, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/twitter/seq2seq_model.ckpt-40\n",
      "\n",
      "<log> Training started </log>\n",
      "\n",
      "Model saved to disk at iteration #10\n",
      "val   loss : 5.197876\n",
      "\n",
      "Model saved to disk at iteration #20\n",
      "val   loss : 4.381861\n",
      "\n",
      "Model saved to disk at iteration #30\n",
      "val   loss : 4.078625\n",
      "\n",
      "Model saved to disk at iteration #40\n",
      "val   loss : 3.977541\n",
      "\n",
      "Model saved to disk at iteration #50\n",
      "val   loss : 4.009060\n",
      "\n",
      "Model saved to disk at iteration #60\n",
      "val   loss : 3.992224\n",
      "\n",
      "Model saved to disk at iteration #70\n",
      "val   loss : 3.957519\n",
      "\n",
      "Model saved to disk at iteration #80\n",
      "val   loss : 3.934952\n",
      "\n",
      "Model saved to disk at iteration #90\n",
      "val   loss : 3.980436\n",
      "\n",
      "Model saved to disk at iteration #100\n",
      "val   loss : 3.943811\n",
      "\n",
      "Model saved to disk at iteration #110\n",
      "val   loss : 3.958645\n",
      "\n",
      "Model saved to disk at iteration #120\n",
      "val   loss : 3.983100\n",
      "\n",
      "Model saved to disk at iteration #130\n",
      "val   loss : 3.847417\n",
      "\n",
      "Model saved to disk at iteration #140\n",
      "val   loss : 3.762195\n",
      "\n",
      "Model saved to disk at iteration #150\n",
      "val   loss : 3.966823\n",
      "\n",
      "Model saved to disk at iteration #160\n",
      "val   loss : 3.929329\n",
      "\n",
      "Model saved to disk at iteration #170\n",
      "val   loss : 3.868466\n",
      "\n",
      "Model saved to disk at iteration #180\n",
      "val   loss : 3.887934\n",
      "\n",
      "Model saved to disk at iteration #190\n",
      "val   loss : 3.793614\n",
      "\n",
      "Model saved to disk at iteration #200\n",
      "val   loss : 3.806081\n",
      "\n",
      "Model saved to disk at iteration #210\n",
      "val   loss : 3.799899\n",
      "\n",
      "Model saved to disk at iteration #220\n",
      "val   loss : 3.721521\n",
      "\n",
      "Model saved to disk at iteration #230\n",
      "val   loss : 3.692606\n",
      "\n",
      "Model saved to disk at iteration #240\n",
      "val   loss : 3.816393\n",
      "\n",
      "Model saved to disk at iteration #250\n",
      "val   loss : 3.629169\n",
      "\n",
      "Model saved to disk at iteration #260\n",
      "val   loss : 3.801875\n",
      "\n",
      "Model saved to disk at iteration #270\n",
      "val   loss : 3.739387\n",
      "\n",
      "Model saved to disk at iteration #280\n",
      "val   loss : 3.738608\n",
      "\n",
      "Model saved to disk at iteration #290\n",
      "val   loss : 3.732111\n",
      "\n",
      "Model saved to disk at iteration #300\n",
      "val   loss : 3.704836\n",
      "\n",
      "Model saved to disk at iteration #310\n",
      "val   loss : 3.689820\n",
      "\n",
      "Model saved to disk at iteration #320\n",
      "val   loss : 3.705944\n",
      "\n",
      "Model saved to disk at iteration #330\n",
      "val   loss : 3.756039\n",
      "\n",
      "Model saved to disk at iteration #340\n",
      "val   loss : 3.758035\n",
      "\n",
      "Model saved to disk at iteration #350\n",
      "val   loss : 3.730286\n",
      "\n",
      "Model saved to disk at iteration #360\n",
      "val   loss : 3.743539\n",
      "\n",
      "Model saved to disk at iteration #370\n",
      "val   loss : 3.643672\n",
      "\n",
      "Model saved to disk at iteration #380\n",
      "val   loss : 3.677556\n",
      "\n",
      "Model saved to disk at iteration #390\n",
      "val   loss : 3.581277\n",
      "\n",
      "Model saved to disk at iteration #400\n",
      "val   loss : 3.721175\n",
      "\n",
      "Model saved to disk at iteration #410\n",
      "val   loss : 3.696716\n",
      "\n",
      "Model saved to disk at iteration #420\n",
      "val   loss : 3.627501\n",
      "\n",
      "Model saved to disk at iteration #430\n",
      "val   loss : 3.663369\n",
      "\n",
      "Model saved to disk at iteration #440\n",
      "val   loss : 3.739985\n",
      "\n",
      "Model saved to disk at iteration #450\n",
      "val   loss : 3.708703\n",
      "\n",
      "Model saved to disk at iteration #460\n",
      "val   loss : 3.689924\n",
      "\n",
      "Model saved to disk at iteration #470\n",
      "val   loss : 3.609164\n",
      "\n",
      "Model saved to disk at iteration #480\n",
      "val   loss : 3.655366\n",
      "\n",
      "Model saved to disk at iteration #490\n",
      "val   loss : 3.665352\n",
      "\n",
      "Model saved to disk at iteration #500\n",
      "val   loss : 3.578096\n",
      "\n",
      "Model saved to disk at iteration #510\n",
      "val   loss : 3.715823\n",
      "\n",
      "Model saved to disk at iteration #520\n",
      "val   loss : 3.702960\n",
      "\n",
      "Model saved to disk at iteration #530\n",
      "val   loss : 3.518382\n",
      "\n",
      "Model saved to disk at iteration #540\n",
      "val   loss : 3.701486\n",
      "\n",
      "Model saved to disk at iteration #550\n",
      "val   loss : 3.649858\n",
      "\n",
      "Model saved to disk at iteration #560\n",
      "val   loss : 3.552243\n",
      "\n",
      "Model saved to disk at iteration #570\n",
      "val   loss : 3.679417\n",
      "\n",
      "Model saved to disk at iteration #580\n",
      "val   loss : 3.693172\n",
      "\n",
      "Model saved to disk at iteration #590\n",
      "val   loss : 3.628090\n",
      "\n",
      "Model saved to disk at iteration #600\n",
      "val   loss : 3.632899\n",
      "\n",
      "Model saved to disk at iteration #610\n",
      "val   loss : 3.680239\n",
      "\n",
      "Model saved to disk at iteration #620\n",
      "val   loss : 3.611809\n",
      "\n",
      "Model saved to disk at iteration #630\n",
      "val   loss : 3.651954\n",
      "\n",
      "Model saved to disk at iteration #640\n",
      "val   loss : 3.640567\n",
      "\n",
      "Model saved to disk at iteration #650\n",
      "val   loss : 3.645937\n",
      "\n",
      "Model saved to disk at iteration #660\n",
      "val   loss : 3.721398\n",
      "\n",
      "Model saved to disk at iteration #670\n",
      "val   loss : 3.705795\n",
      "\n",
      "Model saved to disk at iteration #680\n",
      "val   loss : 3.684556\n",
      "\n",
      "Model saved to disk at iteration #690\n",
      "val   loss : 3.609635\n",
      "\n",
      "Model saved to disk at iteration #700\n",
      "val   loss : 3.738181\n",
      "\n",
      "Model saved to disk at iteration #710\n",
      "val   loss : 3.556502\n",
      "\n",
      "Model saved to disk at iteration #720\n",
      "val   loss : 3.645934\n",
      "\n",
      "Model saved to disk at iteration #730\n",
      "val   loss : 3.719843\n",
      "\n",
      "Model saved to disk at iteration #740\n",
      "val   loss : 3.661910\n",
      "\n",
      "Model saved to disk at iteration #750\n",
      "val   loss : 3.741138\n",
      "\n",
      "Model saved to disk at iteration #760\n",
      "val   loss : 3.712070\n",
      "\n",
      "Model saved to disk at iteration #770\n",
      "val   loss : 3.670346\n",
      "\n",
      "Model saved to disk at iteration #780\n",
      "val   loss : 3.631190\n",
      "\n",
      "Model saved to disk at iteration #790\n",
      "val   loss : 3.767404\n",
      "\n",
      "Model saved to disk at iteration #800\n",
      "val   loss : 3.662627\n",
      "\n",
      "Model saved to disk at iteration #810\n",
      "val   loss : 3.859327\n",
      "\n",
      "Model saved to disk at iteration #820\n",
      "val   loss : 3.661315\n",
      "\n",
      "Model saved to disk at iteration #830\n",
      "val   loss : 3.768403\n",
      "\n",
      "Model saved to disk at iteration #840\n",
      "val   loss : 3.735660\n",
      "\n",
      "Model saved to disk at iteration #850\n",
      "val   loss : 3.649450\n",
      "\n",
      "Model saved to disk at iteration #860\n",
      "val   loss : 3.656224\n",
      "\n",
      "Model saved to disk at iteration #870\n",
      "val   loss : 3.812422\n",
      "\n",
      "Model saved to disk at iteration #880\n",
      "val   loss : 3.701857\n",
      "\n",
      "Model saved to disk at iteration #890\n",
      "val   loss : 3.621604\n",
      "\n",
      "Model saved to disk at iteration #900\n",
      "val   loss : 3.778399\n",
      "\n",
      "Model saved to disk at iteration #910\n",
      "val   loss : 3.588298\n",
      "\n",
      "Model saved to disk at iteration #920\n",
      "val   loss : 3.757850\n",
      "\n",
      "Model saved to disk at iteration #930\n",
      "val   loss : 3.727658\n",
      "\n",
      "Model saved to disk at iteration #940\n",
      "val   loss : 3.730377\n",
      "\n",
      "Model saved to disk at iteration #950\n",
      "val   loss : 3.745195\n",
      "\n",
      "Model saved to disk at iteration #960\n",
      "val   loss : 3.715897\n",
      "\n",
      "Model saved to disk at iteration #970\n",
      "val   loss : 3.572252\n",
      "\n",
      "Model saved to disk at iteration #980\n",
      "val   loss : 3.747344\n",
      "\n",
      "Model saved to disk at iteration #990\n",
      "val   loss : 3.786618\n"
     ]
    }
   ],
   "source": [
    "sess = model.restore_last_session()\n",
    "sess = model.train(train_batch_gen, val_batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/twitter/seq2seq_model.ckpt-40\n",
      "[[  32  110    7 ...    0    0    0]\n",
      " [ 298  187 1076 ...    0    0    0]\n",
      " [  32  354  720 ...    0    0    0]\n",
      " ...\n",
      " [  32   56   21 ...    0    0    0]\n",
      " [ 152   23    0 ...    0    0    0]\n",
      " [  14   60   84 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "sess = model.restore_last_session()\n",
    "print(validX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [2 2 2 ... 0 0 0]\n",
      " [2 2 2 ... 0 0 0]\n",
      " [2 2 2 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "res_idx = model.predict(sess, validX)\n",
    "print(res_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def idx_2_text(result):\n",
    "    # read data control dictionaries\n",
    "    with open('metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "        idx2wd = metadata['idx2w']\n",
    "#        for i in result:\n",
    "            #print(idx2wd[i])\n",
    "    \n",
    "    return idx2wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], []]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "j=0\n",
    "rep_all = [[]]\n",
    "idx2word = idx_2_text(k)\n",
    "for reply in res_idx:\n",
    "#    print(reply)\n",
    "    rep_all.append([])\n",
    "    for k in reply:\n",
    "        #print(k, idx2word[k])\n",
    "        rep_all[j].append(idx2word[k])\n",
    "    #print(rep_all)\n",
    "    j = j+1\n",
    "#    i= i+1\n",
    "    \n",
    "print(rep_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
